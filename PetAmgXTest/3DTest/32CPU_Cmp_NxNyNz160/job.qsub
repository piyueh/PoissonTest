#!/bin/bash

#SBATCH --job-name="PetAmgX"
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16
#SBATCH --time=01:00:00
#SBATCH --output=slurm-job-%j.out
#SBATCH --error=slurm-job-%j.err
#SBATCH --partition=128gb

module load openmpi/1.8/gcc/4.7/cuda
module load cuda/toolkit/6.5
module load boost
module load lapack/gcc
module load blas/gcc

export LM_LICENSE_FILE=/home/pychuang/myPackages/AmgX/amgx_trial-exp-Nov-01-2015.lic
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/pychuang/myPackages/AmgX/lib:/home/pychuang/myPackages/lib
export PETAMGX_DIR=/home/pychuang/PetAmgXTest

mpiexec -display-map -n 32 -map-by ppr:16:node ${PETAMGX_DIR}/bin/PetAmgX -caseName 32CPU_Cmp_NxNyNz160 -platform CPU -Nx 160 -Ny 160 -Nz 160 -cfgFileName solversPetscOptions.info -optFileName perf.txt
mpiexec -display-map -n 32 -map-by ppr:16:node ${PETAMGX_DIR}/bin/PetAmgX -caseName 32CPU_Cmp_Nx200NyNz100 -platform CPU -Nx 200 -Ny 100 -Nz 100 -cfgFileName solversPetscOptions.info -optFileName perf.txt
mpiexec -display-map -n 32 -map-by ppr:16:node ${PETAMGX_DIR}/bin/PetAmgX -caseName 32CPU_Cmp_NxNy200Nz100 -platform CPU -Nx 200 -Ny 200 -Nz 100 -cfgFileName solversPetscOptions.info -optFileName perf.txt
mpiexec -display-map -n 32 -map-by ppr:16:node ${PETAMGX_DIR}/bin/PetAmgX -caseName 32CPU_Cmp_NxNyNz200 -platform CPU -Nx 200 -Ny 200 -Nz 200 -cfgFileName solversPetscOptions.info -optFileName perf.txt
